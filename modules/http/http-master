#!/usr/bin/env bash
## http-master

OUTDIR="${1:-}"
TARGET="${2:-}"

if [ -z "$OUTDIR" ] || [ -z "$TARGET" ]; then
  echo "[!] http-master: Usage: http-master OUTDIR TARGET" >&2
  exit 2
fi

# Resolve ROOTDIR from modules/http/http-master
HMOD="$(cd -- "$(dirname -- "$(readlink -f -- "${BASH_SOURCE[0]}")")" >/dev/null 2>&1 && pwd)"
ROOTDIR="$(cd "$HMOD/../.." >/dev/null 2>&1 && pwd)"
MODDIR="$ROOTDIR/modules"
LISTDIR="$ROOTDIR/lists"

# Colors
. "$ROOTDIR/lib/colors"

# Duplicate stderr onto fd 3 for "live" console output that bypasses dispatch logging
exec 3>&2

mkdir -p "$OUTDIR/http"

# --- HTTP detect (reuses existing helper) ---
eval "$("$MODDIR/http/http-detect" "$OUTDIR" "$TARGET")"

# Print *immediately* on fd 3 so they are not captured in http.log
echo -e "${yellow}[*]${end} ${purple}HTTP candidates:${end} ${blue}${HTTP_CAND:-<none>}${end}" >&3
echo -e "${yellow}[*]${end} ${purple}Verified HTTP ports:${end} ${blue}${HTTP_VERIFIED:-<none>}${end}" >&3

# Persist verified ports for downstream modules (e.g. http-cms)
VERIFIED_FILE="$OUTDIR/http/verified-ports.txt"
: > "$VERIFIED_FILE"
if [ -n "${HTTP_VERIFIED:-}" ]; then
  echo "$HTTP_VERIFIED" \
    | tr ',' '\n' \
    | sed 's/[^0-9]//g' \
    | sed '/^$/d' \
    | sort -n -u > "$VERIFIED_FILE"
fi

# --- HTTP collect per verified port (run in parallel) ---
if [ -n "${HTTP_VERIFIED:-}" ]; then
  pids=()
  for p in $(echo "$HTTP_VERIFIED" | tr ',' ' '); do
    p="${p//[^0-9]/}"
    [ -z "$p" ] && continue
    "$MODDIR/http/http-collect" "$OUTDIR" "$TARGET" "$p" >/dev/null 2>&1 || true &
    pids+=("$!")
  done

  # wait for all http-collect jobs to finish before we read their artifacts
  for pid in "${pids[@]}"; do
    wait "$pid" 2>/dev/null || true
  done
fi

# --- Emails + usernames summary (helper) ---
if [ -n "${HTTP_VERIFIED:-}" ]; then
  "$MODDIR/http/http-emails" \
    "$OUTDIR" \
    "$TARGET" \
    --ports "${HTTP_VERIFIED:-}" \
    --deny-domains "$LISTDIR/deny-domains" \
    || true
else
  echo -e "${yellow}  [!] No HTTP ports to harvest emails from.${end}"
fi

# --- Name harvesting from HTTP content ---
if [ -n "${HTTP_VERIFIED:-}" ]; then
  raw_names="$("$MODDIR/http/http-names" "$OUTDIR" 2>/dev/null || true)"

  echo -e "${yellow}[*]${end} ${purple}Candidate person names from web pages:${end}"

  if [ -n "$raw_names" ]; then
    # Apply deny-names filter HERE (not inside http-names)
    filtered_names="$(printf '%s\n' "$raw_names" \
      | grep -Fvx -f "$LISTDIR/deny-names" 2>/dev/null \
      | sed '/^$/d' \
      | sort -u)"

    if [ -n "$filtered_names" ]; then
      printf '%s\n' "$filtered_names" > "$OUTDIR/http/names-http.txt"

      while IFS= read -r n; do
        echo -e "   ${blue}${n}${end}"
      done <<< "$filtered_names"
    else
      echo -e "   ${yellow}<none>${end}"
    fi
  else
    echo -e "   ${yellow}<none>${end}"
  fi
fi

# --- Domains enrichment ---
# Use the actual target (IP/hostname) as the seed; http/enrich reads all HTTP artifacts
# under \$OUTDIR/http and aggregates hostnames. We don't care if it exits non-zero.
found_hosts="$("$MODDIR/http/enrich" "$OUTDIR" "$TARGET" --max 200 || true)"

echo -e "${yellow}[*]${end} ${purple}Domains found in the web pages:${end}"
if [ -n "$found_hosts" ]; then
  printf '%s\n' "$found_hosts" \
    | sed '/^$/d' \
    | while IFS= read -r h; do
        echo -e "   ${blue}$h${end}"
      done
else
  echo -e "   ${blue}<none>${end}"
fi

# --- Add discovered hosts into /etc/hosts via add-hosts (HTTP mode) ---
if [ -n "${HTTP_VERIFIED:-}" ]; then
  sudo "$ROOTDIR/scripts/add-hosts" \
    "$OUTDIR" \
    "$TARGET" \
    "${HTTP_VERIFIED:-}" \
    --mode http \
    --deny-file "$LISTDIR/deny-domains" \
    --verbose || true
fi

exit 0
